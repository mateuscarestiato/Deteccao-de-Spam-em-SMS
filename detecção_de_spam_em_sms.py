# -*- coding: utf-8 -*-
"""Detec√ß√£o de Spam em SMS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JNXDa6vl_BpERapVV-4YRvFkO6OOm2dH

# **REDES NEURAIS ARTIFICIAIS E DEEP LEARNING**

# **PROPOSTAS AC**

## **PROPOSTA**

### GRUPO

- Mateus Padilha, Ian, Breno, Brenda.

### **PROPOSTA 04**

### GRUPO: Mateus Padilha, Ian, Breno, Brenda.

**Detec√ß√£o de Spam em SMS**

O objetivo do trabalho √© construir um classificador de texto para identificar se uma mensagem SMS √© spam ou n√£o ("ham").

**Dataset:** "SMS Spam Collection Data Set" do reposit√≥rio [UCI Machine Learning](https://archive.ics.uci.edu/dataset/228/sms+spam+collection).

TAREFA

O grupo dever√° implementar uma arquitetura RNN para identificar se uma mensagem SMS √© spam ou n√£o.

# **1. IMPORTA√á√ÉO DE BIBLIOTECAS**

Esta c√©lula re√∫ne todas as bibliotecas necess√°rias para o projeto de detec√ß√£o de spam em mensagens SMS,utilizando Redes Neurais Recorrentes (RNN). A seguir, cada grupo √© descrito com sua fun√ß√£o no pipeline.
"""

# üî¢ Manipula√ß√£o e an√°lise de dados
import numpy as np            # Opera√ß√µes matem√°ticas vetorizadas e manipula√ß√£o de arrays
import pandas as pd           # Leitura, organiza√ß√£o e tratamento de dados tabulares (DataFrames)

# üìä Visualiza√ß√£o
import matplotlib.pyplot as plt   # Cria√ß√£o de gr√°ficos b√°sicos (curvas de aprendizado, ROC, etc.)
import seaborn as sns             # Visualiza√ß√µes estat√≠sticas aprimoradas (ex.: matriz de confus√£o colorida)

# ‚öôÔ∏è Machine Learning Cl√°ssico
from sklearn.model_selection import train_test_split           # Divide o dataset em treino e teste
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc  # M√©tricas de desempenho

# üß† Deep Learning (TensorFlow / Keras)
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.preprocessing.text import Tokenizer       # Tokeniza√ß√£o de texto
from tensorflow.keras.preprocessing.sequence import pad_sequences # Padroniza√ß√£o do comprimento das sequ√™ncias
from tensorflow.keras.models import Sequential                   # Modelo sequencial base
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional  # Camadas da rede
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau             # Callbacks para regulariza√ß√£o e controle do treino

# üìÇ Utilit√°rios
import zipfile    # Manipula√ß√£o de arquivos compactados (.zip)
import os         # Opera√ß√µes de sistema (caminhos e manipula√ß√£o de arquivos)

# ----------------------------------------------------------------------------
# Esta c√©lula prepara o ambiente t√©cnico necess√°rio para todo o pipeline do projeto:
# desde o carregamento e limpeza dos dados at√© a constru√ß√£o, treinamento e avalia√ß√£o
# do modelo RNN para classifica√ß√£o de mensagens SMS em 'spam' ou 'ham'.
# ----------------------------------------------------------------------------

"""
# **2. CARREGAMENTO E EXPLORA√á√ÉO DOS DADOS**
 Nesta etapa, o dataset √© carregado, extra√≠do (caso esteja compactado) e analisado de forma explorat√≥ria. O objetivo √© compreender a estrutura dos dados e verificar sua integridade antes do pr√©-processamento.
"""

# Caminho do arquivo compactado (.zip)
zip_path = 'sms+spam+collection.zip'

# Extra√ß√£o do conte√∫do
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall()  # Extrai todos os arquivos para o diret√≥rio atual

# O arquivo principal esperado chama-se 'SMSSpamCollection'
data_path = 'SMSSpamCollection'

# Leitura dos dados em um DataFrame
# O dataset √© separado por tabula√ß√£o ('\t') e n√£o possui cabe√ßalho
df = pd.read_csv(data_path, sep='\t', header=None, names=['label', 'message'])

# ----------------------------------------------------------------------------
# üîç AN√ÅLISE EXPLORAT√ìRIA
# ----------------------------------------------------------------------------
print("=" * 80)
print("AN√ÅLISE EXPLORAT√ìRIA DOS DADOS")
print("=" * 80)

# Tamanho do dataset (linhas x colunas)
print(f"\nShape do dataset: {df.shape}")

# Primeiras 10 linhas do dataset
print(f"\nPrimeiras linhas:")
print(df.head(10))

# Informa√ß√µes gerais: tipos de dados e presen√ßa de valores nulos
print(f"\nInforma√ß√µes do dataset:")
print(df.info())

# Verifica√ß√£o de valores ausentes
print(f"\nValores nulos:")
print(df.isnull().sum())

# Distribui√ß√£o das classes: n√∫mero de mensagens 'ham' (n√£o spam) e 'spam'
print(f"\nDistribui√ß√£o das classes:")
print(df['label'].value_counts())

# Propor√ß√£o percentual das classes
print(f"\nPropor√ß√£o:")
print(df['label'].value_counts(normalize=True))

# ----------------------------------------------------------------------------
# üìè Estat√≠sticas sobre o comprimento das mensagens
# ----------------------------------------------------------------------------
# Cria√ß√£o de novas colunas com:
# - n√∫mero total de caracteres (message_length)
# - n√∫mero de palavras (word_count)
df['message_length'] = df['message'].apply(len)
df['word_count'] = df['message'].apply(lambda x: len(x.split()))

# Estat√≠sticas descritivas por classe
print(f"\nEstat√≠sticas do tamanho das mensagens (caracteres):")
print(df.groupby('label')['message_length'].describe())

print(f"\nEstat√≠sticas do n√∫mero de palavras:")
print(df.groupby('label')['word_count'].describe())

# ----------------------------------------------------------------------------
# Esta c√©lula garante que o conjunto de dados foi carregado corretamente,
# identifica poss√≠veis desequil√≠brios de classe e fornece informa√ß√µes √∫teis
# para definir estrat√©gias de pr√©-processamento e modelagem.
# ----------------------------------------------------------------------------

"""# **3. VISUALIZA√á√ïES**
Esta c√©lula tem como objetivo representar graficamente as principais caracter√≠sticas do dataset,facilitando a interpreta√ß√£o da distribui√ß√£o de classes e das propriedades textuais das mensagens.

"""

# ============================================================================
# 3. VISUALIZA√á√ïES
# ============================================================================

fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# Distribui√ß√£o de classes
sns.countplot(data=df, x='label', ax=axes[0, 0])
axes[0, 0].set_title('Distribui√ß√£o de Mensagens Spam e Ham', fontsize=14, fontweight='bold')
axes[0, 0].set_xlabel('Classe')
axes[0, 0].set_ylabel('N√∫mero de mensagens')

# Distribui√ß√£o do tamanho das mensagens
sns.boxplot(data=df, x='label', y='message_length', ax=axes[0, 1])
axes[0, 1].set_title('Distribui√ß√£o do Tamanho das Mensagens por Classe', fontsize=14, fontweight='bold')
axes[0, 1].set_xlabel('Classe')
axes[0, 1].set_ylabel('N√∫mero de caracteres')

# Distribui√ß√£o do n√∫mero de palavras
sns.boxplot(data=df, x='label', y='word_count', ax=axes[1, 0])
axes[1, 0].set_title('Distribui√ß√£o do N√∫mero de Palavras por Classe', fontsize=14, fontweight='bold')
axes[1, 0].set_xlabel('Classe')
axes[1, 0].set_ylabel('N√∫mero de palavras')

# Histograma do tamanho das mensagens
df[df['label'] == 'ham']['message_length'].hist(bins=50, alpha=0.6, label='Ham', ax=axes[1, 1])
df[df['label'] == 'spam']['message_length'].hist(bins=50, alpha=0.6, label='Spam', ax=axes[1, 1])
axes[1, 1].set_title('Histograma do Tamanho das Mensagens', fontsize=14, fontweight='bold')
axes[1, 1].set_xlabel('N√∫mero de caracteres')
axes[1, 1].set_ylabel('Frequ√™ncia')
axes[1, 1].legend()

plt.tight_layout()
plt.show()

# Exemplos de mensagens
print("\n" + "=" * 80)
print("EXEMPLOS DE MENSAGENS")
print("=" * 80)
print("\nExemplos de mensagens HAM:")
for i, msg in enumerate(df[df['label'] == 'ham']['message'].head(5), 1):
    print(f"{i}. {msg}")

print("\nExemplos de mensagens SPAM:")
for i, msg in enumerate(df[df['label'] == 'spam']['message'].head(5), 1):
    print(f"{i}. {msg}")

"""A an√°lise dos gr√°ficos gerados no pr√©-processamento indica dois desafios principais para o nosso modelo RNN:

- Desbalanceamento de classes: O n√∫mero de mensagens "ham" (n√£o spam) √© muito maior do que de mensagens "spam", como mostrado no gr√°fico de barras. Isso pode levar o modelo a priorizar a classe majorit√°ria, prejudicando a detec√ß√£o de spam.‚Äã

- Diferen√ßa no comprimento das mensagens: O boxplot mostra que mensagens spam tendem a ser maiores, com menos variabilidade e sem muitos valores extremos. J√° mensagens ham s√£o mais variadas em tamanho, com muitos outliers. Modelos RNN podem ser sens√≠veis a esse tipo de diferen√ßa, especialmente na defini√ß√£o do padding/truncating.‚Äã

# **4. PR√â-PROCESSAMENTO DOS DADOS**
 Esta c√©lula transforma as mensagens de texto em dados num√©ricos que possam ser processados pela RNN.Inclui codifica√ß√£o dos r√≥tulos, tokeniza√ß√£o, padding e divis√£o em conjuntos de treino, valida√ß√£o e teste.
"""

print("\n" + "=" * 80)
print("PR√â-PROCESSAMENTO DOS DADOS")
print("=" * 80)

# ----------------------------------------------------------------------------
# üî¢ Convers√£o dos r√≥tulos
# ----------------------------------------------------------------------------
# Substitui as classes textuais ('ham' e 'spam') por valores num√©ricos bin√°rios (0 e 1)
df['label_encoded'] = df['label'].map({'ham': 0, 'spam': 1})

# ----------------------------------------------------------------------------
# üßπ Prepara√ß√£o de textos e r√≥tulos
# ----------------------------------------------------------------------------
texts = df['message'].values   # Mensagens originais
labels = df['label_encoded'].values  # Labels num√©ricos correspondentes

# ----------------------------------------------------------------------------
# ‚öôÔ∏è Par√¢metros de tokeniza√ß√£o
# ----------------------------------------------------------------------------
MAX_WORDS = 10000  # N√∫mero m√°ximo de palavras no vocabul√°rio
MAX_LEN = 100      # Comprimento m√°ximo (em tokens) de cada sequ√™ncia

# ----------------------------------------------------------------------------
# üß† Tokeniza√ß√£o
# ----------------------------------------------------------------------------
# A tokeniza√ß√£o converte as palavras em inteiros com base na frequ√™ncia.
# Palavras desconhecidas s√£o marcadas com o token especial <OOV> (Out-Of-Vocabulary).
tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token='<OOV>')
tokenizer.fit_on_texts(texts)             # Cria o vocabul√°rio
sequences = tokenizer.texts_to_sequences(texts)  # Converte textos em sequ√™ncias num√©ricas

# ----------------------------------------------------------------------------
# ‚èπÔ∏è Padding
# ----------------------------------------------------------------------------
# As sequ√™ncias s√£o ajustadas para o mesmo comprimento, adicionando zeros ao final das menores.
X = pad_sequences(sequences, maxlen=MAX_LEN, padding='post', truncating='post')
y = labels

# ----------------------------------------------------------------------------
# üìã Relat√≥rio do processamento
# ----------------------------------------------------------------------------
print(f"\nVocabul√°rio total: {len(tokenizer.word_index)}")
print(f"Vocabul√°rio usado (limitado): {MAX_WORDS}")
print(f"Comprimento m√°ximo das sequ√™ncias: {MAX_LEN}")
print(f"Shape de X: {X.shape}")
print(f"Shape de y: {y.shape}")

# ----------------------------------------------------------------------------
# üîÄ Divis√£o em treino, valida√ß√£o e teste
# ----------------------------------------------------------------------------
# O conjunto √© dividido em 70% treino, 15% valida√ß√£o e 15% teste.
X_temp, X_test, y_temp, y_test = train_test_split(
    X, y, test_size=0.15, random_state=42, stratify=y
)
X_train, X_val, y_train, y_val = train_test_split(
    X_temp, y_temp, test_size=0.176, random_state=42, stratify=y_temp
)  # 0.176 * 0.85 ‚âà 0.15

print(f"\nDivis√£o dos dados:")
print(f"Treino: {X_train.shape[0]} amostras ({X_train.shape[0]/len(X)*100:.1f}%)")
print(f"Valida√ß√£o: {X_val.shape[0]} amostras ({X_val.shape[0]/len(X)*100:.1f}%)")
print(f"Teste: {X_test.shape[0]} amostras ({X_test.shape[0]/len(X)*100:.1f}%)")

# ----------------------------------------------------------------------------
# üìä Distribui√ß√£o de classes no conjunto de treino
# ----------------------------------------------------------------------------
unique, counts = np.unique(y_train, return_counts=True)
for u, c in zip(unique, counts):
    print(f"  Classe {u}: {c} amostras ({c/len(y_train)*100:.1f}%)")

# ----------------------------------------------------------------------------
# Interpreta√ß√£o:
# - As mensagens s√£o convertidas para vetores num√©ricos uniformes.
# - As classes foram codificadas de forma bin√°ria para classifica√ß√£o supervisionada.
# - A divis√£o garante conjuntos estratificados e equilibrados.
# ----------------------------------------------------------------------------

"""
# **5. CONSTRU√á√ÉO DO MODELO RNN (LSTM BIDIRECIONAL)**

Esta c√©lula define a arquitetura da Rede Neural Recorrente utilizada para classificar mensagens SMS entre ‚Äúspam‚Äù e ‚Äúham‚Äù. O modelo combina embeddings de palavras com camadas LSTM bidirecionais para capturar o contexto do texto."""

print("\n" + "=" * 80)
print("CONSTRU√á√ÉO DO MODELO")
print("=" * 80)

# ----------------------------------------------------------------------------
# ‚öôÔ∏è Hiperpar√¢metros principais
# ----------------------------------------------------------------------------
EMBEDDING_DIM = 128   # Dimens√£o dos vetores de palavras (representa√ß√£o densa)
LSTM_UNITS = 64       # N√∫mero de unidades da camada LSTM
DROPOUT_RATE = 0.5    # Taxa de dropout para evitar overfitting

# ----------------------------------------------------------------------------
# üß† Arquitetura da Rede Neural
# ----------------------------------------------------------------------------
# - Embedding: transforma palavras em vetores cont√≠nuos de tamanho EMBEDDING_DIM.
# - LSTM Bidirecional: l√™ a sequ√™ncia nos dois sentidos (frente e tr√°s),
#   capturando depend√™ncias contextuais mais amplas.
# - Dropout: desativa aleatoriamente neur√¥nios durante o treino, reduzindo overfitting.
# - Dense (ReLU): camada totalmente conectada intermedi√°ria para refinar a representa√ß√£o.
# - Dense (Sigmoid): camada final que gera probabilidade entre 0 (ham) e 1 (spam).
model = Sequential([
    Embedding(input_dim=MAX_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_LEN),
    Bidirectional(LSTM(LSTM_UNITS, return_sequences=True)),
    Dropout(DROPOUT_RATE),
    Bidirectional(LSTM(LSTM_UNITS // 2)),
    Dropout(DROPOUT_RATE),
    Dense(32, activation='relu'),
    Dropout(DROPOUT_RATE / 2),
    Dense(1, activation='sigmoid')
])

# ----------------------------------------------------------------------------
# ‚ö° Compila√ß√£o do modelo
# ----------------------------------------------------------------------------
# - Otimizador Adam: ajuste autom√°tico da taxa de aprendizado.
# - Fun√ß√£o de perda Binary Crossentropy: adequada para classifica√ß√£o bin√°ria.
# - M√©tricas: acur√°cia, precis√£o e recall (importantes em datasets desbalanceados).
model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]
)

# ----------------------------------------------------------------------------
# üìã Resumo da arquitetura
# ----------------------------------------------------------------------------
print("\nArquitetura do modelo:")
model.summary()

# ----------------------------------------------------------------------------
# Interpreta√ß√£o:
# - As camadas LSTM capturam rela√ß√µes sequenciais entre palavras.
# - O uso de duas camadas bidirecionais melhora a compreens√£o de depend√™ncias de longo prazo.
# - A camada densa final com sigmoid retorna a probabilidade de uma mensagem ser spam.
# ----------------------------------------------------------------------------

"""
# **6. TREINAMENTO DO MODELO**

Esta c√©lula realiza o treinamento da RNN bidirecional usando os dados de treino e valida√ß√£o.S√£o aplicadas t√©cnicas de regulariza√ß√£o e ajuste din√¢mico da taxa de aprendizado para otimizar o desempenho."""

print("\n" + "=" * 80)
print("TREINAMENTO DO MODELO")
print("=" * 80)

# ----------------------------------------------------------------------------
# ‚öôÔ∏è Callbacks ‚Äî mecanismos autom√°ticos de controle durante o treino
# ----------------------------------------------------------------------------
# EarlyStopping: interrompe o treino se a valida√ß√£o parar de melhorar, evitando overfitting.
early_stopping = EarlyStopping(
    monitor='val_loss',          # m√©trica monitorada
    patience=5,                  # n√∫mero de √©pocas sem melhora antes de parar
    restore_best_weights=True,   # restaura os melhores pesos obtidos
    verbose=1
)

# ReduceLROnPlateau: reduz a taxa de aprendizado quando a valida√ß√£o estagna.
reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',   # m√©trica monitorada
    factor=0.5,           # reduz a taxa de aprendizado pela metade
    patience=3,           # n√∫mero de √©pocas sem melhora antes da redu√ß√£o
    min_lr=1e-7,          # limite m√≠nimo da taxa de aprendizado
    verbose=1
)

# ----------------------------------------------------------------------------
# üîß Hiperpar√¢metros de treinamento
# ----------------------------------------------------------------------------
BATCH_SIZE = 32   # N√∫mero de amostras processadas antes da atualiza√ß√£o dos pesos
EPOCHS = 30       # N√∫mero m√°ximo de itera√ß√µes completas sobre o dataset

# ----------------------------------------------------------------------------
# üöÄ Execu√ß√£o do treinamento
# ----------------------------------------------------------------------------
history = model.fit(
    X_train, y_train,
    batch_size=BATCH_SIZE,
    epochs=EPOCHS,
    validation_data=(X_val, y_val),
    callbacks=[early_stopping, reduce_lr],
    verbose=1
)

# ----------------------------------------------------------------------------
# Interpreta√ß√£o:
# - O modelo ajusta seus pesos em cada √©poca com base no erro de treino.
# - O EarlyStopping garante que o treino pare antes de sobreajustar os dados.
# - O ReduceLROnPlateau permite ajustes finos quando a melhoria desacelera.
# - As m√©tricas de valida√ß√£o guiam o ponto √≥timo de generaliza√ß√£o.
# ----------------------------------------------------------------------------

"""
# **7. VISUALIZA√á√ÉO DO TREINAMENTO**
Plota as curvas de loss, acur√°cia, precis√£o e recall para treino e valida√ß√£o."""

# ============================================================================
# 7. VISUALIZA√á√ÉO DO TREINAMENTO
# ============================================================================

fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# Loss
axes[0, 0].plot(history.history['loss'], label='Treino', linewidth=2)
axes[0, 0].plot(history.history['val_loss'], label='Valida√ß√£o', linewidth=2)
axes[0, 0].set_title('Loss durante o Treinamento', fontsize=14, fontweight='bold')
axes[0, 0].set_xlabel('√âpoca')
axes[0, 0].set_ylabel('Loss')
axes[0, 0].legend()
axes[0, 0].grid(True, alpha=0.3)

# Accuracy
axes[0, 1].plot(history.history['accuracy'], label='Treino', linewidth=2)
axes[0, 1].plot(history.history['val_accuracy'], label='Valida√ß√£o', linewidth=2)
axes[0, 1].set_title('Acur√°cia durante o Treinamento', fontsize=14, fontweight='bold')
axes[0, 1].set_xlabel('√âpoca')
axes[0, 1].set_ylabel('Acur√°cia')
axes[0, 1].legend()
axes[0, 1].grid(True, alpha=0.3)

# Precision
axes[1, 0].plot(history.history['precision'], label='Treino', linewidth=2)
axes[1, 0].plot(history.history['val_precision'], label='Valida√ß√£o', linewidth=2)
axes[1, 0].set_title('Precis√£o durante o Treinamento', fontsize=14, fontweight='bold')
axes[1, 0].set_xlabel('√âpoca')
axes[1, 0].set_ylabel('Precis√£o')
axes[1, 0].legend()
axes[1, 0].grid(True, alpha=0.3)

# Recall
axes[1, 1].plot(history.history['recall'], label='Treino', linewidth=2)
axes[1, 1].plot(history.history['val_recall'], label='Valida√ß√£o', linewidth=2)
axes[1, 1].set_title('Recall durante o Treinamento', fontsize=14, fontweight='bold')
axes[1, 1].set_xlabel('√âpoca')
axes[1, 1].set_ylabel('Recall')
axes[1, 1].legend()
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

"""
# **8. AVALIA√á√ÉO NO CONJUNTO DE TESTE**

Esta c√©lula avalia o desempenho final do modelo no conjunto de teste,
verificando sua capacidade de generaliza√ß√£o e interpretando os principais indicadores de desempenho."""

print("\n" + "=" * 80)
print("AVALIA√á√ÉO NO CONJUNTO DE TESTE")
print("=" * 80)

# Predi√ß√µes
y_pred_proba = model.predict(X_test)
y_pred = (y_pred_proba > 0.5).astype(int).flatten()

# M√©tricas
test_loss, test_acc, test_precision, test_recall = model.evaluate(X_test, y_test, verbose=0)
test_f1 = 2 * (test_precision * test_recall) / (test_precision + test_recall)

print(f"\nM√©tricas no conjunto de teste:")
print(f"  Loss: {test_loss:.4f}")
print(f"  Acur√°cia: {test_acc:.4f}")
print(f"  Precis√£o: {test_precision:.4f}")
print(f"  Recall: {test_recall:.4f}")
print(f"  F1-Score: {test_f1:.4f}")

print(f"\nRelat√≥rio de Classifica√ß√£o:")
print(classification_report(y_test, y_pred, target_names=['Ham', 'Spam']))

# Matriz de Confus√£o
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Ham', 'Spam'],
            yticklabels=['Ham', 'Spam'])
plt.title('Matriz de Confus√£o', fontsize=16, fontweight='bold')
plt.ylabel('Valor Real')
plt.xlabel('Valor Predito')
plt.show()

# Curva ROC
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('Taxa de Falsos Positivos', fontsize=12)
plt.ylabel('Taxa de Verdadeiros Positivos', fontsize=12)
plt.title('Curva ROC', fontsize=16, fontweight='bold')
plt.legend(loc="lower right")
plt.grid(True, alpha=0.3)
plt.show()

"""
# **9. AN√ÅLISE DE ERROS**

Esta c√©lula identifica e exibe exemplos de classifica√ß√µes incorretas feitas pelo modelo.O objetivo √© entender onde a RNN ainda falha e quais padr√µes textuais confundem o classificador."""

print("\n" + "=" * 80)
print("AN√ÅLISE DE ERROS")
print("=" * 80)

# Encontrar √≠ndices dos erros
error_indices = np.where(y_pred != y_test)[0]
print(f"\nTotal de erros: {len(error_indices)} ({len(error_indices)/len(y_test)*100:.2f}%)")

# Falsos Positivos (Ham classificado como Spam)
fp_indices = np.where((y_pred == 1) & (y_test == 0))[0]
print(f"Falsos Positivos: {len(fp_indices)}")

# Falsos Negativos (Spam classificado como Ham)
fn_indices = np.where((y_pred == 0) & (y_test == 1))[0]
print(f"Falsos Negativos: {len(fn_indices)}")

# Reconstruir mensagens originais para an√°lise
test_indices = X_temp.shape[0] - X_val.shape[0]
original_test_messages = df.iloc[-X_test.shape[0]:]['message'].values

if len(fp_indices) > 0:
    print(f"\nExemplos de Falsos Positivos (Ham ‚Üí Spam):")
    for i, idx in enumerate(fp_indices[:3], 1):
        print(f"{i}. Confian√ßa: {y_pred_proba[idx][0]:.4f}")
        print(f"   Mensagem: {original_test_messages[idx]}\n")

if len(fn_indices) > 0:
    print(f"\nExemplos de Falsos Negativos (Spam ‚Üí Ham):")
    for i, idx in enumerate(fn_indices[:3], 1):
        print(f"{i}. Confian√ßa: {y_pred_proba[idx][0]:.4f}")
        print(f"   Mensagem: {original_test_messages[idx]}\n")

"""
# **10. TESTE COM NOVAS MENSAGENS**
Esta c√©lula aplica o modelo treinado em mensagens in√©ditas para avaliar seu comportamento fora do dataset original e verificar sua capacidade de generaliza√ß√£o."""

# ============================================================================
# 10. TESTE COM NOVAS MENSAGENS
# ============================================================================

print("\n" + "=" * 80)
print("TESTE COM NOVAS MENSAGENS")
print("=" * 80)

def predict_message(message, model, tokenizer, max_len):
    """Prediz se uma mensagem √© spam ou ham"""
    sequence = tokenizer.texts_to_sequences([message])
    padded = pad_sequences(sequence, maxlen=max_len, padding='post', truncating='post')
    prediction = model.predict(padded, verbose=0)[0][0]
    label = 'SPAM' if prediction > 0.5 else 'HAM'
    confidence = prediction if prediction > 0.5 else 1 - prediction
    return label, confidence

# Mensagens de teste
test_messages = [
    "Hey, how are you doing today?",
    "Congratulations! You've won a free iPhone. Click here to claim your prize now!",
    "Can we meet for coffee tomorrow?",
    "URGENT! Your account will be closed. Reply with your password immediately.",
    "Thanks for the dinner last night, it was great!",
    "FREE entry to win a ¬£1000 prize! Text WIN to 12345",
]

print("\nPredi√ß√µes para novas mensagens:\n")
for i, msg in enumerate(test_messages, 1):
    label, confidence = predict_message(msg, model, tokenizer, MAX_LEN)
    print(f"{i}. Mensagem: '{msg}'")
    print(f"   Predi√ß√£o: {label} (Confian√ßa: {confidence*100:.2f}%)\n")

"""REFER√äNCIAS:

[1] SMS Spam Collection Data Set - UCI Machine Learning Repository. Dispon√≠vel em: https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection

[2] TensorFlow. LSTM for text classification. Dispon√≠vel em: https://www.tensorflow.org/tutorials/text/text_classification_rnn

[3] Perplexity AI

[4] Claude Ai

[5] ChatGPT

[4] https://www.geeksforgeeks.org/deep-learning/sms-spam-detection-using-tensorflow-in-python/

[5] https://sist.sathyabama.ac.in/sist_naac/documents/1.3.4/1822-b.e-cse-batchno-109.pdf

[6] Almeida, T.A., Hidalgo, J.M.G., Yamakami, A. (2011).
    "Contributions to the Study of SMS Spam Filtering: New Collection and Results"
    Proceedings of the 11th ACM Symposium on Document Engineering (DOCENG'11)
    
[7] Hochreiter, S., & Schmidhuber, J. (1997).
    "Long Short-Term Memory". Neural Computation, 9(8), 1735-1780.
    
[8] Schuster, M., & Paliwal, K. K. (1997).
    "Bidirectional Recurrent Neural Networks". IEEE Transactions on Signal Processing.
    
[9] Chollet, F. et al. (2015). Keras.
    GitHub Repository: https://github.com/keras-team/keras
    
[10] Abadi, M. et al. (2015). TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems.
    Software available from tensorflow.org
    
[11] Goodfellow, I., Bengio, Y., & Courville, A. (2016).
    "Deep Learning". MIT Press. http://www.deeplearningbook.org
    
[12] UCI Machine Learning Repository. (2012).
    "SMS Spam Collection Data Set"
    https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection

Alguns dos prompts recuperados:

"Baixei o dataset, agora me de um direcionamento para eu come√ßar a resolu√ß√£o do meu problema."

‚ÄúDocumente esse trabalho a seguir.‚Äù

"Preciso balancear as minhas classes." (forcenemos as imagens de sa√≠da)

"Quero agora graficos para visualiza√ß√£o do meu problema."

"Preciso fazer a an√°lise explorat√≥ria e pr√©-processamento dos dados."
"""